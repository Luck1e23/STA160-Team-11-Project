{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Luck1e23/STA160-Team-11-Project/blob/Tina/STA160_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XygmfDJQrVKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fec207c-be73-4054-f2f8-462d860cced3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchxrayvision\n",
        "!pip install iterative-stratification"
      ],
      "metadata": {
        "id": "fO57a1XV-g1B",
        "outputId": "f40de978-5249-430c-e425-45ca4245b338",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchxrayvision in /usr/local/lib/python3.12/dist-packages (1.4.0)\n",
            "Requirement already satisfied: torch>=1 in /usr/local/lib/python3.12/dist-packages (from torchxrayvision) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.12/dist-packages (from torchxrayvision) (0.24.0+cu126)\n",
            "Requirement already satisfied: scikit-image>=0.16 in /usr/local/lib/python3.12/dist-packages (from torchxrayvision) (0.25.2)\n",
            "Requirement already satisfied: tqdm>=4 in /usr/local/lib/python3.12/dist-packages (from torchxrayvision) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1 in /usr/local/lib/python3.12/dist-packages (from torchxrayvision) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1 in /usr/local/lib/python3.12/dist-packages (from torchxrayvision) (2.2.2)\n",
            "Requirement already satisfied: requests>=1 in /usr/local/lib/python3.12/dist-packages (from torchxrayvision) (2.32.4)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchxrayvision) (11.3.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.12/dist-packages (from torchxrayvision) (2.37.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1->torchxrayvision) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1->torchxrayvision) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1->torchxrayvision) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=1->torchxrayvision) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=1->torchxrayvision) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=1->torchxrayvision) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=1->torchxrayvision) (2025.11.12)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16->torchxrayvision) (1.16.3)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16->torchxrayvision) (3.5)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16->torchxrayvision) (2025.10.16)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16->torchxrayvision) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16->torchxrayvision) (0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1->torchxrayvision) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1->torchxrayvision) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1->torchxrayvision) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1->torchxrayvision) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1->torchxrayvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1->torchxrayvision) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1->torchxrayvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1->torchxrayvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1->torchxrayvision) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1->torchxrayvision) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1->torchxrayvision) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1->torchxrayvision) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1->torchxrayvision) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1->torchxrayvision) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1->torchxrayvision) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1->torchxrayvision) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1->torchxrayvision) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1->torchxrayvision) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1->torchxrayvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1->torchxrayvision) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1->torchxrayvision) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1->torchxrayvision) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1->torchxrayvision) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1->torchxrayvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1->torchxrayvision) (3.0.3)\n",
            "Requirement already satisfied: iterative-stratification in /usr/local/lib/python3.12/dist-packages (0.1.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from iterative-stratification) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from iterative-stratification) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from iterative-stratification) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->iterative-stratification) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->iterative-stratification) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Packages"
      ],
      "metadata": {
        "id": "UMaZLheYLESe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torch.optim import Adam\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
        "from pathlib import Path\n",
        "\n",
        "# Visualization tools\n",
        "import torchvision\n",
        "import torchvision.transforms.v2 as transforms\n",
        "import torchvision.transforms.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Pre-trained Model: torchxrayvision\n",
        "import torchxrayvision as xrv\n",
        "import skimage\n"
      ],
      "metadata": {
        "id": "5Tz4bz8OYrF0",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-Trained Model"
      ],
      "metadata": {
        "id": "gqqJPSC5LG-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XRV Pathology Classifiers :: NIH chest X-ray8\n",
        "xrv_model = xrv.models.DenseNet(weights=\"densenet121-res224-nih\")"
      ],
      "metadata": {
        "id": "20kXR5Qo-mRm",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File Paths"
      ],
      "metadata": {
        "id": "T4zf1j7TLNcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "file_path = '/content/drive/Shareddrives/STA_160/dataset/Data_Entry_2017.csv'\n",
        "train_val = '/content/drive/Shareddrives/STA_160/dataset/train_val_list.txt'\n",
        "test = '/content/drive/Shareddrives/STA_160/dataset/test_list.txt'\n",
        "dataset_root = '/content/dataset_raw'           # Original dataset (copied from Drive)\n",
        "resized_root = '/content/dataset_resized'   # Where resized images will be saved"
      ],
      "metadata": {
        "id": "FB3KHeztx0U4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a 224 x 224 Resized Dataset"
      ],
      "metadata": {
        "id": "Sd3bmnIbLaeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Old Code for resizing the images before making a dataset class\n",
        "# Purpose was to speed up the running of the code\n",
        "'''\n",
        "# Target Size\n",
        "IMG_SIZE = (224, 224)\n",
        "\n",
        "for root, dirs, files in os.walk(dataset_root):\n",
        "    rel_path = os.path.relpath(root, dataset_root)\n",
        "    save_dir = os.path.join(resized_root, rel_path)\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    for f in files:\n",
        "        if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            img_path = os.path.join(root, f)\n",
        "\n",
        "            # Load and convert to grayscale (preserve pixel values)\n",
        "            img = Image.open(img_path).convert(\"L\")\n",
        "            arr = np.array(img).astype(np.uint8)\n",
        "\n",
        "            # Resize using bilinear interpolation\n",
        "            img_resized = Image.fromarray(arr).resize(IMG_SIZE, Image.BILINEAR)\n",
        "\n",
        "            # Build output filename (always PNG)\n",
        "            base = os.path.splitext(f)[0]\n",
        "            out_path = os.path.join(save_dir, base + \".png\")\n",
        "\n",
        "            # Save as PNG to avoid JPEG compression artifacts\n",
        "            img_resized.save(out_path, format=\"PNG\")\n",
        "\n",
        "# Zip the resized dataset\n",
        "!zip -r -q /content/NIH_resized.zip /content/dataset_resized\n",
        "\n",
        "# Copy and upload to the shared drive\n",
        "!cp /content/NIH_resized.zip /content/drive/Shareddrives/STA_160/\n",
        "'''\n"
      ],
      "metadata": {
        "id": "GcFnymr_lVx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "hAly1Xa8LWFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Class\n",
        "class NIHXrays(Dataset):\n",
        "    def __init__(self, file_path, dataset_root, list_file=None):\n",
        "        self.data = pd.read_csv(file_path)\n",
        "        self.dataset_root = dataset_root\n",
        "\n",
        "        # Optional filtering\n",
        "        if list_file:\n",
        "            with open(list_file, 'r') as f:\n",
        "                image_list = {line.strip() for line in f.readlines()}\n",
        "            self.data = self.data[self.data['Image Index'].isin(image_list)].reset_index(drop=True)\n",
        "\n",
        "        # Create label map\n",
        "        all_labels = set()\n",
        "        for labels in self.data['Finding Labels']:\n",
        "            for l in labels.split('|'):\n",
        "                all_labels.add(l.strip())\n",
        "\n",
        "        self.all_labels = sorted(all_labels)\n",
        "        self.label_map = {label: i for i, label in enumerate(self.all_labels)}\n",
        "\n",
        "        # Build multi-hot label vectors\n",
        "        self.finding_labels = []\n",
        "        for labels in self.data['Finding Labels']:\n",
        "            vec = torch.zeros(len(self.all_labels))\n",
        "            for l in labels.split('|'):\n",
        "                if l.strip() in self.label_map:\n",
        "                    vec[self.label_map[l.strip()]] = 1.0\n",
        "            self.finding_labels.append(vec)\n",
        "\n",
        "        self.finding_labels = torch.stack(self.finding_labels).float()\n",
        "\n",
        "        # Recursively map image filenames to full paths\n",
        "        self.image_map = {}\n",
        "        for img_path in Path(dataset_root).rglob(\"*.png\"):\n",
        "            self.image_map[img_path.name] = str(img_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.data.iloc[idx]['Image Index']\n",
        "\n",
        "        if img_name not in self.image_map:\n",
        "            raise FileNotFoundError(f\"Image {img_name} not found.\")\n",
        "\n",
        "        # Load Image\n",
        "        img = Image.open(self.image_map[img_name]).convert(\"L\")\n",
        "        img = np.array(img).astype(np.float32) # PIL image --> NumPy [224,224]\n",
        "\n",
        "        # XRV normalizaiton\n",
        "        img = xrv.datasets.normalize(img, maxval=255)\n",
        "\n",
        "        #   NumPy --> Tensor\n",
        "        img = torch.from_numpy(img).unsqueeze(0) # [1, 224, 224]\n",
        "\n",
        "        label = self.finding_labels[idx].float()\n",
        "\n",
        "        return img, label, img_name"
      ],
      "metadata": {
        "id": "6YWDipkc_-RM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_val_data = NIHXrays(file_path, resized_root, list_file = train_val)\n",
        "test_data = NIHXrays(file_path, resized_root, list_file = test)"
      ],
      "metadata": {
        "id": "Uj_QxJY4iETn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Label Stratified Split for Training and Validation"
      ],
      "metadata": {
        "id": "wvP1Ou_ELoge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to NumPy for the stratifier\n",
        "y = train_val_data.finding_labels.numpy()   # shape: [N, C]\n",
        "X = np.arange(len(train_val_data))          # dummy feature array\n",
        "\n",
        "#Stratified Splitting\n",
        "msss = MultilabelStratifiedShuffleSplit(\n",
        "    n_splits=1,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "#only want the first split & get train and validation indices\n",
        "train_idx, valid_idx = next(msss.split(X, y))\n",
        "\n",
        "# Get train and validation datasets based on the indices\n",
        "train_data = Subset(train_val_data, train_idx)\n",
        "valid_data = Subset(train_val_data, valid_idx)"
      ],
      "metadata": {
        "id": "oYKK4hGP79zX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Augmentation and Data Loader"
      ],
      "metadata": {
        "id": "sk5ynvR3KsJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders for training and validation\n",
        "n = 32\n",
        "train_loader = DataLoader(train_data, batch_size=n, shuffle=True, num_workers=2, pin_memory=True)\n",
        "train_N = len(train_loader.dataset)\n",
        "valid_loader = DataLoader(valid_data, batch_size=n, num_workers=2, pin_memory=True)\n",
        "valid_N = len(valid_loader.dataset)\n",
        "\n",
        "# Data Augmentation\n",
        "IMG_WIDTH, IMG_HEIGHT = (224, 224)\n",
        "rand_transforms = transforms.Compose([\n",
        "    transforms.RandomRotation(25),\n",
        "    transforms.RandomResizedCrop((IMG_WIDTH, IMG_HEIGHT), scale = (0.8, 1), ratio = (1, 1)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2)\n",
        "])"
      ],
      "metadata": {
        "id": "zJNbI834Ki-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Function & Freezing Base Layers of Pre-Trained Model"
      ],
      "metadata": {
        "id": "-2qpavjqMcu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XRV pathologies\n",
        "xrv_labels = ['Atelectasis', 'Consolidation', 'Infiltration', 'Pneumothorax',\n",
        "              'Edema', 'Emphysema', 'Fibrosis', 'Effusion', 'Pneumonia',\n",
        "              'Pleural_Thickening', 'Cardiomegaly', 'Nodule', 'Mass', 'Hernia',\n",
        "              '', '', '', '']\n",
        "\n",
        "# Your 15 dataset labels (example)\n",
        "dataset_labels = ['Atelectasis', 'Consolidation', 'Infiltration', 'Pneumothorax',\n",
        "                  'Edema', 'Emphysema', 'Fibrosis', 'Effusion', 'Pneumonia',\n",
        "                  'Pleural_Thickening', 'Cardiomegaly', 'Nodule', 'Mass', 'Hernia', 'No findings']\n",
        "\n",
        "# Create mapping from dataset_labels → xrv_labels index\n",
        "xrv_index_map = [xrv_labels.index(lbl) if lbl in xrv_labels else None for lbl in dataset_labels]\n",
        "\n",
        "\n",
        "class XRV_Finetune(nn.Module):\n",
        "    def __init__(self, base_model, num_classes, xrv_index_map):\n",
        "        super().__init__()\n",
        "        self.base = base_model\n",
        "        self.xrv_index_map = xrv_index_map\n",
        "        self.in_features = len(xrv_index_map)  # 15, including \"No findings\"\n",
        "\n",
        "        # Freeze backbone\n",
        "        for p in self.base.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        # New classification head\n",
        "        self.classifier = nn.Linear(self.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.base(x)  # [B, 18] from DenseNet-121\n",
        "        xrv_features = []\n",
        "\n",
        "        for idx in self.xrv_index_map:\n",
        "            if idx is not None:\n",
        "                xrv_features.append(out[:, idx].unsqueeze(1)) # Selects entire batch of specific disease and goes from [Batch size] --> [Batch size, 1]\n",
        "            else:\n",
        "                # placeholder zero for \"No findings\", classifier can learn weights\n",
        "                xrv_features.append(torch.zeros(out.size(0), 1, device=out.device))\n",
        "\n",
        "        xrv_features = torch.cat(xrv_features, dim=1)  # [Batch size, 15]\n",
        "        out = self.classifier(xrv_features)            # [Batch size, 15]\n",
        "\n",
        "        return out\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#Pre-trained Model\n",
        "xrv_model = xrv.models.DenseNet(weights=\"densenet121-res224-nih\")\n",
        "\n",
        "# Freeze\n",
        "xrv_model.requires_grad_(False)\n",
        "print(\"XRV frozen\")\n",
        "\n",
        "\n",
        "N_CLASSES = 15\n",
        "\n",
        "my_model = XRV_Finetune(xrv_model, N_CLASSES, xrv_index_map = xrv_index_map).to(device)\n",
        "\n",
        "# Finding class weights\n",
        "def compute_pos_weights(subset):\n",
        "\n",
        "    # Get all labels that are in the subset\n",
        "    full_dataset = subset.dataset\n",
        "    labels = full_dataset.finding_labels[subset.indices].numpy()\n",
        "\n",
        "    pos_counts = labels.sum(axis = 0) #counts how many of each disease present in the dataset\n",
        "    neg_counts = (labels == 0).sum(axis = 0) # counts how many times each disease was NOT present in the dataset\n",
        "    pos_weight = neg_counts / (pos_counts + 1e-6) # Ratio of negatives to positives. 1e-6 to prevent dividing by 0.\n",
        "\n",
        "    return torch.tensor(pos_weight, dtype = torch.float32) #Convert to tensor\n",
        "\n",
        "# Choosing loss function\n",
        "pos_weight = compute_pos_weights(train_data).to(device) # Addressing class imbalance: add more weight to rare diseases\n",
        "pos_weight = torch.clamp(pos_weight, max=30) # Because one of the values was > 600\n",
        "\n",
        "loss_function = nn.BCEWithLogitsLoss(pos_weight = pos_weight) #Since data is multi-label binary classification\n",
        "optimizer = Adam(my_model.classifier.parameters(), lr=1e-2) #increase classifiers learning rate\n",
        "my_model = my_model.to(device)"
      ],
      "metadata": {
        "id": "PI1Xo8H-wLAZ",
        "outputId": "a47c63b3-c4e0-42ba-9c28-310e31f62cd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XRV frozen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Performance Metrics"
      ],
      "metadata": {
        "id": "OMv3zOb_Mth1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, roc_auc_score, precision_recall_curve\n",
        "import torch\n",
        "\n",
        "def pr_curve_thresholds(y_true, y_prob):\n",
        "    num_labels = y_true.shape[1]\n",
        "    thresholds = np.zeros(num_labels)\n",
        "\n",
        "    for i in range(num_labels):\n",
        "        p, r, t = precision_recall_curve(y_true[:, i], y_prob[:, i])\n",
        "        f1 = 2 * p * r / (p + r + 1e-9)\n",
        "        thresholds[i] = t[np.argmax(f1)]\n",
        "\n",
        "    return thresholds\n",
        "\n",
        "def compute_f1(y_true, y_prob, thresholds):\n",
        "\n",
        "    y_true = y_true.cpu()\n",
        "    y_prob = y_prob.cpu()\n",
        "\n",
        "    # Converts thresholds into tensors and match the shape of y_prob\n",
        "    thresholds = torch.tensor(thresholds, dtype = y_prob.dtype)\n",
        "    y_pred = (y_prob >= thresholds).int()\n",
        "\n",
        "    return f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "\n",
        "def compute_auc(y_true, y_prob):\n",
        "\n",
        "    y_true = y_true.detach().cpu().numpy() #binary labels\n",
        "    y_prob = y_prob.detach().cpu().numpy() #sigmoid probabilities\n",
        "\n",
        "    return roc_auc_score(y_true, y_prob, average = \"macro\")"
      ],
      "metadata": {
        "id": "WzjW-d9LznmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Validation Functions"
      ],
      "metadata": {
        "id": "5Mt02VCfM5aZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, loss_fn, device, check_grad=False):\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    for imgs, labels, _ in train_loader:\n",
        "\n",
        "        # Applying Data Augmentation\n",
        "        augmented_imgs = []\n",
        "\n",
        "        for img in imgs:  # iterate batch\n",
        "            pil_img = transforms.ToPILImage()(img)       # convert tensor → PIL\n",
        "            pil_img = rand_transforms(pil_img)           # apply augmentation\n",
        "            aug_img = transforms.ToTensor()(pil_img)     # back to tensor\n",
        "            augmented_imgs.append(aug_img)\n",
        "\n",
        "        imgs = torch.stack(augmented_imgs).to(device)    # [B,1,224,224]\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward\n",
        "        outputs = model(imgs)\n",
        "\n",
        "        # Compute loss\n",
        "        batch_loss = loss_fn(outputs, labels)\n",
        "\n",
        "        # Backprop\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += batch_loss.item()\n",
        "\n",
        "        # Probability for AUC\n",
        "        probs = torch.sigmoid(outputs)\n",
        "\n",
        "        all_probs.append(probs.detach().cpu())\n",
        "        all_labels.append(labels.detach().cpu())\n",
        "\n",
        "    if check_grad:\n",
        "        print(\"Last Gradient:\")\n",
        "        for p in model.parameters():\n",
        "            if p.grad is not None:\n",
        "                print(p.grad)\n",
        "\n",
        "    # Compute AUROC at end of epoch\n",
        "    all_probs = torch.cat(all_probs)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    auc = compute_auc(all_labels, all_probs)\n",
        "    epoch_loss = total_loss / train_N\n",
        "\n",
        "    print(\"Train - Loss: {:.4f}, AUC: {:.4f}\".format(epoch_loss, auc))\n",
        "\n",
        "    return epoch_loss, auc\n",
        "\n",
        "def validate(model, valid_loader, loss_fn, device):\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels, _ in valid_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Compute loss\n",
        "            outputs = model(imgs)\n",
        "            batch_loss = loss_fn(outputs, labels)\n",
        "\n",
        "            total_loss += batch_loss.item()\n",
        "\n",
        "            # Get probabilities for F1 and AUC\n",
        "            probs = torch.sigmoid(outputs)\n",
        "\n",
        "            all_probs.append(probs.detach().cpu())\n",
        "            all_labels.append(labels.detach().cpu())\n",
        "\n",
        "    #Store all probabilities and labels\n",
        "    all_probs = torch.cat(all_probs)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    # Compute loss for every epoch\n",
        "    epoch_loss = total_loss / valid_N\n",
        "\n",
        "    return epoch_loss, all_probs, all_labels\n"
      ],
      "metadata": {
        "id": "uprQdGCuEloA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fit Model and Evaluations"
      ],
      "metadata": {
        "id": "w_JGqpUkM9lD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}\")\n",
        "\n",
        "    train_loss, train_auc = train(\n",
        "        model=my_model,\n",
        "        train_loader=train_loader,\n",
        "        optimizer=optimizer,\n",
        "        loss_fn=loss_function,\n",
        "        device=device,\n",
        "        check_grad=False\n",
        "    )\n",
        "\n",
        "    valid_loss, valid_probs, valid_labels = validate(\n",
        "        model=my_model,\n",
        "        valid_loader=valid_loader,\n",
        "        loss_fn=loss_function,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    #Find thresholds per label from validation\n",
        "    thresholds = pr_curve_thresholds(valid_labels, valid_probs)\n",
        "\n",
        "    # Find F1 and AUC for validation\n",
        "    valid_f1 = compute_f1(valid_labels, valid_probs, thresholds)\n",
        "    valid_auc = compute_auc(valid_labels, valid_probs)\n",
        "\n",
        "    print(\"Valid - Loss: {:.4f}, F1: {:.4f}, AUC: {:.4f}\".format(valid_loss, valid_f1, valid_auc))"
      ],
      "metadata": {
        "id": "OHnKfia9VJlH",
        "outputId": "36c67fd5-89e8-41a8-bf53-587075df03b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n",
            "Train - Loss: 0.0323, AUC: 0.5026\n",
            "Valid - Loss: 0.0324, F1: 0.1258, AUC: 0.4996\n",
            "\n",
            "Epoch 2\n",
            "Train - Loss: 0.0323, AUC: 0.5034\n",
            "Valid - Loss: 0.0326, F1: 0.1258, AUC: 0.5001\n",
            "\n",
            "Epoch 3\n",
            "Train - Loss: 0.0322, AUC: 0.5137\n",
            "Valid - Loss: 0.0331, F1: 0.1258, AUC: 0.5003\n",
            "\n",
            "Epoch 4\n",
            "Train - Loss: 0.0322, AUC: 0.5125\n",
            "Valid - Loss: 0.0332, F1: 0.1258, AUC: 0.5002\n",
            "\n",
            "Epoch 5\n",
            "Train - Loss: 0.0322, AUC: 0.5085\n",
            "Valid - Loss: 0.0337, F1: 0.1258, AUC: 0.5003\n",
            "\n",
            "Epoch 6\n",
            "Train - Loss: 0.0323, AUC: 0.5110\n",
            "Valid - Loss: 0.0338, F1: 0.1258, AUC: 0.5007\n",
            "\n",
            "Epoch 7\n",
            "Train - Loss: 0.0322, AUC: 0.5121\n",
            "Valid - Loss: 0.0343, F1: 0.1258, AUC: 0.5003\n",
            "\n",
            "Epoch 8\n",
            "Train - Loss: 0.0322, AUC: 0.5147\n",
            "Valid - Loss: 0.0345, F1: 0.1258, AUC: 0.5004\n",
            "\n",
            "Epoch 9\n",
            "Train - Loss: 0.0323, AUC: 0.5119\n",
            "Valid - Loss: 0.0343, F1: 0.1258, AUC: 0.5002\n",
            "\n",
            "Epoch 10\n",
            "Train - Loss: 0.0322, AUC: 0.5117\n",
            "Valid - Loss: 0.0347, F1: 0.1258, AUC: 0.5003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-Tuning Model"
      ],
      "metadata": {
        "id": "m4tefZi0NB7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine Tuning the model\n",
        "\n",
        "# Unfreeze the last block model\n",
        "unfreeze_blocks = [\"denseblock4\", \"transition3\", \"norm5\"]\n",
        "for name, param in my_model.base.features.named_parameters():\n",
        "    if any(block in name for block in unfreeze_blocks):\n",
        "        param.requires_grad = True\n",
        "\n",
        "#Assign different learning rates\n",
        "optimizer = Adam([\n",
        "    {'params': [p for p in my_model.base.parameters() if p.requires_grad], 'lr': 1e-6},\n",
        "    {'params': my_model.classifier.parameters(), 'lr': 1e-3}\n",
        "])"
      ],
      "metadata": {
        "id": "ekjJKhKrtkp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 8\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}\")\n",
        "\n",
        "    train_loss, train_auc = train(\n",
        "        model=my_model,\n",
        "        train_loader=train_loader,\n",
        "        optimizer=optimizer,\n",
        "        loss_fn=loss_function,\n",
        "        device=device,\n",
        "        check_grad=False\n",
        "    )\n",
        "\n",
        "    valid_loss, valid_probs, valid_labels = validate(\n",
        "        model=my_model,\n",
        "        valid_loader=valid_loader,\n",
        "        loss_fn=loss_function,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    #Find thresholds per label from validation\n",
        "    thresholds = pr_curve_thresholds(valid_labels, valid_probs)\n",
        "\n",
        "    # Find F1 and AUC for validation\n",
        "    valid_f1 = compute_f1(valid_labels, valid_probs, thresholds)\n",
        "    valid_auc = compute_auc(valid_labels, valid_probs)\n",
        "\n",
        "    print(\"Valid - Loss: {:.4f}, F1: {:.4f}, AUC: {:.4f}\".format(valid_loss, valid_f1, valid_auc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "MhTHbz5FSmiM",
        "outputId": "bc80e500-be5c-4040-f3a0-a9aa0ee98342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3894811005.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nEpoch {epoch+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     train_loss, train_auc = train(\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3525922175.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, loss_fn, device, check_grad)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3359389637.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, 18] from DenseNet-121\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mxrv_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchxrayvision/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"op_threshs\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_threshs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_threshs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchxrayvision/models.py\u001b[0m in \u001b[0;36mop_norm\u001b[0;34m(outputs, op_threshs)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0moutputs_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_leq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_leq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mop_threshs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_leq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[0;31m# scale outputs greater than thresh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0moutputs_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_gt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_gt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mop_threshs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_gt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mConcatenate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_TensorLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tensor\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m ) -> Callable[Concatenate[_TensorLike, _P], \"Tensor\"]:\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_TensorLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Tensor\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xrv_model.pathologies)\n",
        "print(len(xrv_model.pathologies))\n"
      ],
      "metadata": {
        "id": "iNhAh6T8bKD-",
        "outputId": "b2979d44-c7d6-4347-fd55-e5b9a8c9e18b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Atelectasis', 'Consolidation', 'Infiltration', 'Pneumothorax', 'Edema', 'Emphysema', 'Fibrosis', 'Effusion', 'Pneumonia', 'Pleural_Thickening', 'Cardiomegaly', 'Nodule', 'Mass', 'Hernia', '', '', '', '']\n",
            "18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pos_weight)"
      ],
      "metadata": {
        "id": "yWjAw9UvW2Co",
        "outputId": "2111e78b-649f-4936-8b18-9012463c8b93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([  9.4831,  49.7850,  29.4665,  62.6799,   9.0377,  59.2962,  68.7782,\n",
            "        658.2381,   5.2956,  20.7195,   0.7123,  17.3705,  38.0632,  96.2191,\n",
            "         31.6509], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_model)\n"
      ],
      "metadata": {
        "id": "7vbWvP4WfY-x",
        "outputId": "b962785f-ed81-489a-b174-a91319a95abd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XRV_Finetune(\n",
            "  (base): XRV-DenseNet121-densenet121-res224-nih\n",
            "  (classifier): Linear(in_features=14, out_features=15, bias=True)\n",
            ")\n"
          ]
        }
      ]
    }
  ]
}