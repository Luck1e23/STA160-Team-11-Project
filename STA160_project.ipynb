{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Luck1e23/STA160-Team-11-Project/blob/Tina/STA160_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XygmfDJQrVKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6904e0c-79ba-4527-c80c-833897b1553d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Visualization tools\n",
        "import torchvision\n",
        "import torchvision.transforms.v2 as transforms\n",
        "import torchvision.transforms.functional as F\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "5Tz4bz8OYrF0",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torchvision.datasets.MNIST(\"./\",download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvLAPWIske_Q",
        "outputId": "3a0d721a-d58e-4a05-d77e-eb5a1b09a38e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 57.4MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.71MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.9MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 9.95MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: ./\n",
              "    Split: Train"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Dataset class, NIHXrays, which contains the normalized pixel values of all the images\n",
        "\n",
        "class NIHXrays(Dataset):\n",
        "    def __init__(self, file_path, dataset_root, list_file = None):\n",
        "        self.data = pd.read_csv(file_path)\n",
        "        self.dataset_root = dataset_root\n",
        "\n",
        "        if list_file:\n",
        "            # Open the file and convert into a list of image names\n",
        "            with open(list_file, 'r') as f:\n",
        "                image_list = [line.strip() for line in f.readlines()]\n",
        "            # Keep only rows that match the listed image names\n",
        "            self.data = self.data[self.data['Image Index'].isin(image_list)].reset_index(drop = True)\n",
        "\n",
        "        # Label map\n",
        "        all_labels = set()\n",
        "        for labels in self.data['Finding Labels']:\n",
        "            for l in labels.split('|'):\n",
        "                all_labels.add(l.strip())\n",
        "        self.all_labels = sorted(list(all_labels))\n",
        "        self.label_map = {label: idx for idx, label in enumerate(self.all_labels)}\n",
        "\n",
        "        self.finding_labels = []\n",
        "        for labels in self.data['Finding Labels']:\n",
        "            vec = torch.zeros(len(self.all_labels))\n",
        "            # Obtaining all the unique disease names\n",
        "            for l in labels.split('|'):\n",
        "                l = l.strip()\n",
        "                if l in self.label_map:\n",
        "                    vec[self.label_map[l]] = 1.0 # Mark as 1 if that is the disease found\n",
        "            self.finding_labels.append(vec)\n",
        "\n",
        "        # Image map\n",
        "        self.image_map = {}\n",
        "        # Go through all subfolders\n",
        "        for root, dirs, files in os.walk(dataset_root):\n",
        "            # Only keep 'images' folders\n",
        "            if os.path.basename(root) == \"images\":\n",
        "                for f in files:\n",
        "                    self.image_map[f] = os.path.join(root, f)\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor() # Converts image to tensor and normalizes pixel values\n",
        "            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        # Get total number of samples\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        row = self.data.iloc[idx]\n",
        "        img_name = row['Image Index']\n",
        "        finding_label = self.finding_labels[idx]\n",
        "\n",
        "        # Load and transform the image\n",
        "        path = self.image_map.get(img_name)\n",
        "        if not path:\n",
        "            raise FileNotFoundError(f\"Image {img_name} not found in dataset root.\")\n",
        "        img = self.transform(Image.open(path).convert('L')) # Convert to grayscale, resize, and a tensor\n",
        "\n",
        "        return img, finding_label, img_name"
      ],
      "metadata": {
        "id": "QK5_0np4R18e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/Shareddrives/STA_160/dataset/Data_Entry_2017.csv'\n",
        "dataset_root = '/content/drive/Shareddrives/STA_160/dataset'\n",
        "train_val = '/content/drive/Shareddrives/STA_160/dataset/train_val_list.txt'\n",
        "test = '/content/drive/Shareddrives/STA_160/dataset/test_list.txt'\n",
        "\n",
        "\n",
        "train_val_data = NIHXrays(file_path, dataset_root, list_file = train_val)\n",
        "test_data = NIHXrays(file_path, dataset_root, list_file = test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uj_QxJY4iETn",
        "outputId": "3cbb77df-d485-4163-9d48-defe1e81df69"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the train_val_data into training and validation datasets\n",
        "train_data, valid_data = torch.utils.data.random_split(train_val_data, [0.8, 0.2])\n",
        "\n",
        "# Create DataLoaders for training and validation\n",
        "n = 32\n",
        "train_loader = DataLoader(train_data, batch_size=n, shuffle=True)\n",
        "train_N = len(train_loader.dataset)\n",
        "valid_loader = DataLoader(valid_data, batch_size=n)\n",
        "valid_N = len(valid_loader.dataset)\n",
        "\n",
        "\n",
        "# Data Augmentation\n",
        "IMG_WIDTH, IMG_HEIGHT = (224, 224)\n",
        "rand_transforms = transforms.Compose([\n",
        "    transforms.RandomRotation(25),\n",
        "    transforms.RandomResizedCrop((IMG_WIDTH, IMG_HEIGHT), scale = (0.8, 1), ratio = (1, 1)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2)\n",
        "])"
      ],
      "metadata": {
        "id": "oYKK4hGP79zX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PI1Xo8H-wLAZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}